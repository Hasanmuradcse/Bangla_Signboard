{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b43d47",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (2.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (3.8.1)\n",
      "\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (2022.3.15)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from transformers[sentencepiece]) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from aiohttp->datasets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\ppg\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccea947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf1342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import os\n",
    "# root=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\"\n",
    "# all_contents = load_dataset(\"json\", data_files=os.path.join(root,\"dataV2.jsonl\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f19068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard_folder=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\shard2\"\n",
    "# sharding={}\n",
    "# total_shard=12\n",
    "# for x in range(0,total_shard):\n",
    "#   sharding[x]=all_contents.shard(num_shards=total_shard,index=x)\n",
    "#   sharding[x].to_json(os.path.join(shard_folder,f\"shard{x}.jsonl\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfe3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc373714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a5fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shard_folder=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\shard2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a9c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81030f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_training_corpus(raw_datasets2):\n",
    "#     return (\n",
    "#         raw_datasets2[i : i + 1000][\"incorrect\"]\n",
    "#         for i in tqdm.tqdm(range(0, len(raw_datasets2), 1000))\n",
    "#     )\n",
    "\n",
    "# for x in tqdm.tqdm(range(0,12)):\n",
    "#     raw_datasets=load_dataset(\"json\", data_files=os.path.join(shard_folder,f\"shard{x}.jsonl\"), split=\"train\")\n",
    "#     training_corpus = get_training_corpus(raw_datasets)\n",
    "#     tokenizer = tokenizer.train_new_from_iterator(training_corpus, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e492b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300e644",
   "metadata": {},
   "source": [
    "Whole tokenizer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f1d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b8b0ac7455455dee\n",
      "Reusing dataset json (C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\json\\default-b8b0ac7455455dee\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    }
   ],
   "source": [
    "folder=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\"\n",
    "raw_datasets=load_dataset(\"json\", data_files=os.path.join(folder,\"dataV2.jsonl\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11006648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['incorrect', 'correct'],\n",
       "    num_rows: 16265806\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce2150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "li=list(raw_datasets['incorrect'])\n",
    "li2=list(raw_datasets['correct'])\n",
    "li.extend(li2)\n",
    "print(len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8d9c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "104f4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32531612\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57b1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "d = Dataset.from_dict({'data':li})\n",
    "# raw_datasets2=DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8111faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 32532/32532 [30:16<00:00, 17.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus(raw_datasets2):\n",
    "    return (\n",
    "        raw_datasets2[i : i + 1000][\"data\"]\n",
    "        for i in tqdm.tqdm(range(0, len(raw_datasets2), 1000))\n",
    "    )\n",
    "training_corpus = get_training_corpus(d)\n",
    "tokenizer = tokenizer.train_new_from_iterator(training_corpus, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019c99b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\tokenizer_whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9e8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_training_corpus(raw_datasets2):\n",
    "#     return (\n",
    "#         raw_datasets2[i : i + 1000][\"correct\"]\n",
    "#         for i in tqdm.tqdm(range(0, len(raw_datasets2), 1000))\n",
    "#     )\n",
    "\n",
    "# folder=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\"\n",
    "# raw_datasets=load_dataset(\"json\", data_files=os.path.join(folder,\"dataV2.jsonl\"), split=\"train\")\n",
    "# training_corpus = get_training_corpus(raw_datasets)\n",
    "# tokenizer = tokenizer.train_new_from_iterator(training_corpus, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d419126",
   "metadata": {},
   "source": [
    "### Hasan murad tokenizer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013a09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\tokenizer_whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48010557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2d7bbc252fb0dd97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\json\\default-2d7bbc252fb0dd97\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39223080beda498d9288e753ead541ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e98f082b1e43e3afbfa208a1c55f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\json\\default-2d7bbc252fb0dd97\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "path=r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\murad_bhai_data.jsonl\"\n",
    "raw_datasets=load_dataset(\"json\", data_files=path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a85d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['incorrect', 'correct'],\n",
       "    num_rows: 113133\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e299da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226266\n"
     ]
    }
   ],
   "source": [
    "li=list(raw_datasets['incorrect'])\n",
    "li2=list(raw_datasets['correct'])\n",
    "li.extend(li2)\n",
    "print(len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa58c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "d = Dataset.from_dict({'data':li})\n",
    "# raw_datasets2=DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74eda9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:02<00:00, 85.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus(raw_datasets2):\n",
    "    return (\n",
    "        raw_datasets2[i : i + 1000][\"data\"]\n",
    "        for i in tqdm.tqdm(range(0, len(raw_datasets2), 1000))\n",
    "    )\n",
    "training_corpus = get_training_corpus(d)\n",
    "tokenizer = tokenizer.train_new_from_iterator(training_corpus, 40500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbad542e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole2\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole2\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Administrator\\\\Desktop\\\\my research(Shakib)\\\\error correction model\\\\tokenizer_whole2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(r\"C:\\Users\\Administrator\\Desktop\\my research(Shakib)\\error correction model\\tokenizer_whole2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826aaa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
